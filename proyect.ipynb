{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''LOAD EMBEDDING'''\n",
    "def load_embedding(embedding_name):\n",
    "    File =  open(embedding_name)\n",
    "    Model = {}\n",
    "    for line in File:\n",
    "        split = line.split()\n",
    "        #print(split[0])\n",
    "        word = split[0]\n",
    "        #word = split[2]\n",
    "        #print(word)\n",
    "        embedding = np.array([float(val) for val in split[1:]])\n",
    "        if len(embedding) == 300:\n",
    "            Model[word] = embedding\n",
    "\n",
    "    File.close()\n",
    "    print(embedding_name +\" model loaded!\")\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''SANITY TEST'''\n",
    "def sanity_test(embedding_1, embedding_2, output_file):\n",
    "    synsetList = []\n",
    "    for synset in embedding_1:\n",
    "        print(synset)\n",
    "        if synset.endswith(\"n\"):\n",
    "            synsetList.append(synset)\n",
    "\n",
    "    randomSample = np.random.choice(Synset, 10)\n",
    "\n",
    "    cont = 0\n",
    "    progress = 0\n",
    "    for rnd in randomSample:\n",
    "        maxScore = 0\n",
    "        maxSynset = \"FOO\"\n",
    "        top = []\n",
    "        for words in embedding_2:\n",
    "            similarity = cosine_similarity([embedding_1[rnd]], [embedding_2[words]])\n",
    "            top.append({'word':words, 'sim':similarity})\n",
    "            if(len(top)>10):\n",
    "                top = sorted(top, key=itemgetter('sim'), reverse=True)\n",
    "                top.pop()\n",
    "        \n",
    "        progress = progress + 1\n",
    "        print(\"...\",progress,\"%\") \n",
    "        with open('/home/josu/Desktop/wn_Mapped->GN.txt', 'a') as the_file:\n",
    "            the_file.write(\"Most similar synset to \" + str(rnd) + \"\\n\")\n",
    "            for elem in top:\n",
    "                the_file.write(str(elem.get('word')) + \" \" + str(elem.get('sim')) + \"\\n\")\n",
    "            the_file.write(\"\\n\")\n",
    "    percentage = cont/len(randomSample)*100\n",
    "    print(\"Percentage of success: \", percentage, \"%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''WORD SEARCH'''\n",
    "def word_synset_comparison(embedding_1, embedding_2, word_to_search, filepath):\n",
    "    word_to_search = \"party\"\n",
    "    '''\n",
    "    search = []\n",
    "    errorc = 0\n",
    "    for wSearch in gnModel:\n",
    "        try:\n",
    "            #if re.match(wSearch, '/^party$/'):\n",
    "            if wSearch.endswith(word_to_search) and len(wSearch)==len(word_to_search):\n",
    "                search.append(gnModel[wSearch])\n",
    "        except:\n",
    "            errorc = errorc +1\n",
    "\n",
    "    print(\"Number of errors \"+ str(errorc))\n",
    "    '''\n",
    "    top = []\n",
    "    for synset in embedding_1:\n",
    "        similarity = cosine_similarity([embedding_2[word_to_search]], [embedding_1[synset]])\n",
    "        top.append({'synset':synset, 'sim':similarity})\n",
    "        if(len(top)>10):\n",
    "            top = sorted(top, key=itemgetter('sim'), reverse=True)\n",
    "            top.pop()\n",
    "    print(str(top.get('synset')))\n",
    "    with open(filepath, 'a') as the_file:\n",
    "        the_file.write(\"Most similar synset to \" + str(word_to_search) + \"\\n\")\n",
    "        for elem in top:\n",
    "            the_file.write(str(elem.get('synset')) + \" \" + str(elem.get('sim')) + \"\\n\")\n",
    "        the_file.write(\"\\n\")\n",
    "    print(\"Results in \" + str(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''WORD SEARCH'''\n",
    "def word_synset_comparison_csv(embedding_1, embedding_2, wordList, csvpath, dictpath=\"NULL\"):\n",
    "    for word in wordList:\n",
    "        top = []\n",
    "        for synset in embedding_1:\n",
    "            similarity = cosine_similarity([embedding_2[word]], [embedding_1[synset]])\n",
    "            top.append({'synset':synset, 'sim':similarity})\n",
    "            if(len(top)>10):\n",
    "                top = sorted(top, key=itemgetter('sim'), reverse=True)\n",
    "                top.pop()\n",
    "        if dictpath != \"NULL\":\n",
    "            top = translator(top, dictpath)\n",
    "\n",
    "        with open(csvpath, \"a\") as output:\n",
    "            head = \"Most similar synset to \" + str(word).upper() + \"\\n\"\n",
    "            output.write(head)\n",
    "            dictWriter = csv.DictWriter(output, fieldnames=['synset', 'sim'])\n",
    "            for element in top:\n",
    "                dictWriter.writerow(element)\n",
    "            output.write(\"\\n\")\n",
    "            \n",
    "    print(\"Results in \" + str(csvpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_translator(inputFile, dictionary, outputFile):\n",
    "    dictFile = open(dictionary)\n",
    "    dictArray = []\n",
    "    for line in dictFile:\n",
    "        split = line.split()\n",
    "        dictArray.append({'from':split[0], 'to':split[1]})\n",
    "    dictFile.close()\n",
    "    \n",
    "    targetFile = open(inputFile)\n",
    "    for line in targetFile:\n",
    "        split = line.split()\n",
    "        toWrite = []\n",
    "        for word in split:\n",
    "            #if word.endswith(\"-n\"):\n",
    "            if re.search('-[a-z]$', word):\n",
    "                error = 1\n",
    "                for elem in dictArray:\n",
    "                    if word == elem.get('from'):\n",
    "                        error = 0\n",
    "                        toWrite.append(elem.get('to'))\n",
    "                if error == 1:\n",
    "                    print(\"Could not translate \" + word)\n",
    "                    toWrite.append(word)\n",
    "            else:\n",
    "                toWrite.append(word)\n",
    "        with open(outputFile, 'a') as output:\n",
    "            for res in toWrite:\n",
    "                output.write(res + \" \")\n",
    "            output.write(\"\\n\")\n",
    "    targetFile.close()\n",
    "    \n",
    "    print(\"Output file in \" + outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translator(to_translate, dictionary):\n",
    "    dictFile = open(dictionary)\n",
    "    dictArray = []\n",
    "    for line in dictFile:\n",
    "        split = line.split()\n",
    "        dictArray.append({'from':split[0], 'to':split[1]})\n",
    "    dictFile.close()\n",
    "    \n",
    "    if len(to_translate) < 1:\n",
    "        print(\"Can't translate, length 0\")\n",
    "        return -1\n",
    "    \n",
    "    else:\n",
    "        transList = []\n",
    "        for element in to_translate:\n",
    "            error = 1\n",
    "            for elem in dictArray:\n",
    "                if element.get('synset') == elem.get('from'):\n",
    "                    error = 0\n",
    "                    transList.append({'synset':elem.get('to'), 'sim':element.get('sim')})\n",
    "            if error == 1:\n",
    "                transList.append({'synset':element.get('synset'), 'sim':element.get('sim')})\n",
    "                print(str(element.get('synset')) + \"not found, will not be translated\")\n",
    "        \n",
    "        return transList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathWN_GN = '/home/josu/Desktop/vecmap-master/wn_synsets_Mapped.txt'\n",
    "pathGN = '/home/josu/Desktop/vecmap-master/GN_1,5M_Mapped.txt'\n",
    "pathWN_GN_UKB = '/home/josu/Desktop/vecmap-master/wn_synsets_Mapped_GN+UKB.txt'\n",
    "pathGN_UKB = '/home/josu/Desktop/vecmap-master/GN_1,5M_Mapped_UKB_Mapped_WN.txt'\n",
    "pathWN_GN_UKB_PPA = '/home/josu/Desktop/vecmap-master/wn_synsets_Mapped_GN+UKB+PPA.txt'\n",
    "pathGN_UKB_PPA = '/home/josu/Desktop/vecmap-master/GN_1,5M_Mapped_UKB_PPA_Mapped_WN.txt'\n",
    "pathWN_Dict = '/home/josu/Downloads/wnet30_dict.synset-word.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/josu/Desktop/vecmap-master/wn_synsets_Mapped.txt model loaded!\n",
      "-----------------------------------------------------------\n",
      "/home/josu/Desktop/vecmap-master/GN_1,5M_Mapped.txt model loaded!\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "wnEmbedding = load_embedding(pathWN_GN)\n",
    "gnEmbedding = load_embedding(pathGN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/josu/Desktop/vecmap-master/wn_synsets_Mapped_GN+UKB.txt model loaded!\n",
      "-----------------------------------------------------------\n",
      "/home/josu/Desktop/vecmap-master/GN_1,5M_Mapped_UKB_Mapped_WN.txt model loaded!\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "wnEmbedding = load_embedding(pathWN_GN_UKB)\n",
    "gnEmbedding = load_embedding(pathGN_UKB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/josu/Desktop/vecmap-master/wn_synsets_Mapped_GN+UKB+PPA.txt model loaded!\n",
      "-----------------------------------------------------------\n",
      "/home/josu/Desktop/vecmap-master/GN_1,5M_Mapped_UKB_PPA_Mapped_WN.txt model loaded!\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "wnEmbedding = load_embedding(pathWN_GN_UKB_PPA)\n",
    "gnEmbedding = load_embedding(pathGN_UKB_PPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_test(wnEmbedding, gnEmbedding,'/home/josu/Desktop/prueba.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in /home/josu/Desktop/party_comparison_WN_GN+UKB.txt\n"
     ]
    }
   ],
   "source": [
    "word_synset_comparison(wnEmbedding, gnEmbedding, \"party\", \"/home/josu/Desktop/basura.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results in /home/josu/Desktop/WN_GN.csv\n"
     ]
    }
   ],
   "source": [
    "word_synset_comparison_csv(wnEmbedding, gnEmbedding, ['party', 'bank', 'man', 'mole', 'book', 'wood'], \"/home/josu/Desktop/WN_GN.csv\", pathWN_Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
